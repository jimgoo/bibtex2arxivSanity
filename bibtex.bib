%% Created using Papers on Fri, 08 Sep 2017.
%% http://papersapp.com/papers/

@article{Chen:2016tp,
author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
title = {{InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets}},
journal = {arXiv.org},
year = {2016},
eprint = {1606.03657v1},
eprinttype = {arxiv},
month = jun,
date-added = {2017-07-08T21:13:40GMT},
date-modified = {2017-09-08T05:48:11GMT},
abstract = {This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing fully supervised methods.},
url = {http://arxiv.org/abs/1606.03657v1}
}

@article{Held:2016wy,
author = {Held, David and Thrun, Sebastian and Savarese, Silvio},
title = {{Learning to Track at 100 FPS with Deep Regression Networks}},
journal = {arXiv.org},
year = {2016},
eprint = {1604.01802v2},
eprinttype = {arxiv},
eprintclass = {cs.CV},
month = apr,
annote = {To appear in European Conference on Computer Vision (ECCV) 2016},
abstract = {Machine learning techniques are often used in computer vision due to their ability to leverage large amounts of training data to improve performance. Unfortunately, most generic object trackers are still trained from scratch online and do not benefit from the large number of videos that are readily available for offline training. We propose a method for offline training of neural networks that can track novel objects at test-time at 100 fps. Our tracker is significantly faster than previous methods that use neural networks for tracking, which are typically very slow to run and not practical for real-time applications. Our tracker uses a simple feed-forward network with no online training required. The tracker learns a generic relationship between object motion and appearance and can be used to track novel objects that do not appear in the training set. We test our network on a standard tracking benchmark to demonstrate our tracker's state-of-the-art performance. Further, our performance improves as we add more videos to our offline training set. To the best of our knowledge, our tracker is the first neural-network tracker that learns to track generic objects at 100 fps.},
url = {http://arxiv.org/abs/1604.01802v2}
}
